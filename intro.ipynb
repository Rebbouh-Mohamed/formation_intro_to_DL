{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning - Part 1  \n",
    "### Understanding Neural Networks from Scratch  \n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this beginner-friendly guide to deep learning! üöÄ  \n",
    "In this notebook, we‚Äôll explore **neural networks**, how they work, and how we can use them for function approximation.  \n",
    "\n",
    "---\n",
    "\n",
    "üîπ **What you'll learn:**  \n",
    "‚úîÔ∏è Basics of Neural Networks  \n",
    "‚úîÔ∏è How Neurons Work  \n",
    "‚úîÔ∏è Activation Functions  \n",
    "‚úîÔ∏è Training with Backpropagation  \n",
    "\n",
    "Let's dive in! üß†üí°  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Function? ü§î  \n",
    "\n",
    "Before we dive into neural networks, let's first understand **functions**.  \n",
    "\n",
    "A **function** is a system that takes an input and produces an output.  \n",
    "Mathematically, we can write it as:  \n",
    "\n",
    "\\[\n",
    "y = f(x)\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\( x \\) is the input,  \n",
    "- \\( f \\) is the function,  \n",
    "- \\( y \\) is the output.  \n",
    "\n",
    "For example, if we have:\n",
    "\n",
    "\\[\n",
    "y = 2x + 3\n",
    "\\]\n",
    "\n",
    "and we input \\( x = 4 \\), we get:\n",
    "\n",
    "\\[\n",
    "y = 2(4) + 3 = 11\n",
    "\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Function and Its Relation to Neural Networks? ü§î  \n",
    "\n",
    "Let's start with a question:  \n",
    "\n",
    "**Suppose we have some inputs and their corresponding outputs. Can we figure out the function that produced them?**  \n",
    "\n",
    "At first, it might seem impossible. If we don‚Äôt know the function, how can we find it? But !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Answer\n",
    "the answer is **yes!** We can approximate the function that maps inputs to outputs using a technique called **function approximation**.  \n",
    "\n",
    "This is exactly what a **neural network** does! Instead of manually writing a function, we use a system that learns the relationship between inputs and outputs automatically.  \n",
    "\n",
    "### Example: Understanding Function Approximation  \n",
    "\n",
    "Imagine we have the following input-output pairs:  \n",
    "\n",
    "| Input (x) | Output (y) |\n",
    "|-----------|-----------|\n",
    "| 1         | 3         |\n",
    "| 2         | 5         |\n",
    "| 3         | 7         |\n",
    "\n",
    "Can you see a pattern? The function behind this data is:  \n",
    "\n",
    "\\[\n",
    "y = 2x + 1\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we had **thousands** of inputs and outputs, and the function was much more complex? Finding the function manually would be very difficult.  \n",
    "\n",
    "This is where **neural networks** come in! They help us learn the function that best fits the data **without us needing to define it explicitly**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neuron? üß†  \n",
    "\n",
    "Now that we understand the idea of functions and function approximation, let‚Äôs break down the **basic building block of a neural network**‚Äîthe **neuron**.  \n",
    "\n",
    "A **neuron** is just a function! It takes some inputs, applies some calculations, and produces an output.  \n",
    "\n",
    "### How Does a Neuron Work?  \n",
    "\n",
    "Each neuron takes multiple inputs and applies two key components:  \n",
    "\n",
    "1. **Weights (\\( w \\))**: Each input is multiplied by a weight, which determines how important that input is.  \n",
    "2. **Bias (\\( b \\))**: A bias is added to shift the function up or down, helping the neuron learn more flexible patterns.  \n",
    "\n",
    "Mathematically, a neuron works like this:  \n",
    "\n",
    "\\[\n",
    "\\output = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + ... + (w_n \\cdot x_n) + b\n",
    "\\]\n",
    "\n",
    "Then, we apply a **special function** (called an **activation function**) to make the output more useful. We‚Äôll talk about activation functions soon!  \n",
    "\n",
    "---\n",
    "\n",
    "## What is a Neural Network? ü§ñ  \n",
    "\n",
    "A **neural network** is just a **combination of many neurons working together**!  \n",
    "\n",
    "Instead of a single neuron, we **stack multiple neurons into layers** to create a powerful system that can learn complex functions.  \n",
    "\n",
    "### Structure of a Neural Network  \n",
    "\n",
    "A neural network is made up of **three types of layers**:  \n",
    "\n",
    "1. **Input Layer** üéØ ‚Äì Takes in the raw data (e.g., pixels from an image, words from text).  \n",
    "2. **Hidden Layers** üîÑ ‚Äì The \"thinking\" part of the network, where neurons process information and learn patterns.  \n",
    "3. **Output Layer** üéØ ‚Äì Produces the final result (e.g., predicting a number, classifying an image).  \n",
    "\n",
    "üí° **Think of a neural network like a team of neurons working together!** Each neuron contributes a small part, and together they can approximate almost any function.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why Do We Need Weights and Biases?  \n",
    "\n",
    "- **Weights** control the importance of each input, allowing the network to adjust and learn.  \n",
    "- **Bias** helps shift the function, allowing the network to learn patterns that wouldn‚Äôt be possible with just weights alone.  \n",
    "\n",
    "Together, weights and biases **help the network learn from data and improve its accuracy**!  \n",
    "\n",
    "In the next section, we‚Äôll discuss **why we need activation functions** and how they help neurons make better decisions. üöÄ  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Neurons Can Only Approximate Linear Functions üòü  \n",
    "\n",
    "Now that we know what a neuron is, let's talk about a big limitation.  \n",
    "\n",
    "A **single neuron** is just a **linear function** because it only does this:  \n",
    "\n",
    "\\[\n",
    "\\text{output} = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + ... + (w_n \\cdot x_n) + b\n",
    "\\]\n",
    "\n",
    "This is just like drawing a straight line on a graph! üìà  \n",
    "\n",
    "But in deep learning, we want to represent **more complex patterns**, like recognizing objects in images or understanding speech. These problems need **non-linear** functions, not just straight lines.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Activation Functions üöÄ  \n",
    "\n",
    "To solve this, we use something called an **activation function**.  \n",
    "\n",
    "An activation function **adds non-linearity** to our neurons, so they can learn complex relationships instead of just straight lines. It allows the network to combine neurons in a way that can approximate **any function**, no matter how complicated!  \n",
    "\n",
    "### Most Common Activation Functions  \n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)** üî•  \n",
    "   - The most commonly used activation function.  \n",
    "   - Formula:  \n",
    "     \\[\n",
    "     f(x) =\n",
    "     \\begin{cases} \n",
    "     x, & x > 0 \\\\\n",
    "     0, & x \\leq 0\n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - Simply returns the input if it's positive, otherwise outputs zero.  \n",
    "   - Helps networks learn faster and prevents unnecessary complexity.  \n",
    "\n",
    "2. **Sigmoid** üìâ  \n",
    "   - Formula:  \n",
    "     \\[\n",
    "     f(x) = \\frac{1}{1 + e^{-x}}\n",
    "     \\]\n",
    "   - Squashes values between **0 and 1**, making it useful for probabilities.  \n",
    "   - Used in older networks but can cause problems like vanishing gradients.  \n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)** üìà  \n",
    "   - Formula:  \n",
    "     \\[\n",
    "     f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "     \\]\n",
    "   - Similar to sigmoid but outputs values between **-1 and 1**, making it better for balanced data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Are Activation Functions So Important?  \n",
    "\n",
    "Without activation functions, **no matter how deep our network is, it will still behave like a simple linear function!** That means it wouldn't be able to learn complex relationships.  \n",
    "\n",
    "But by adding activation functions, we allow our network to model **non-linear patterns**, making deep learning so powerful.  \n",
    "\n",
    "### Key Takeaway:  \n",
    "üîë **Activation functions give neurons the ability to learn beyond straight lines, allowing deep learning to approximate any function!**  \n",
    "\n",
    "Next, we‚Äôll discuss **how neural networks learn** using backpropagation! ‚ö°  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Neural Networks Learn ü§ñ  \n",
    "\n",
    "Now that we understand neurons and activation functions, let‚Äôs talk about how a neural network **learns**.  \n",
    "\n",
    "Neural networks learn in **three main steps**:  \n",
    "\n",
    "1. **Feedforward** üöÄ (Making a prediction)  \n",
    "2. **Loss Calculation** ‚öñÔ∏è (Measuring the error)  \n",
    "3. **Backpropagation & Optimization** üîÑ (Fixing the errors)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Feedforward ‚Äì Making Predictions  \n",
    "\n",
    "Imagine we have an input (like an image of a cat üê±), and we want our neural network to recognize it.  \n",
    "\n",
    "### What happens?  \n",
    "1. The input is passed through the network **layer by layer**.  \n",
    "2. Each neuron processes the input using **weights, biases, and activation functions**.  \n",
    "3. The network finally **produces an output** (e.g., \"90% chance it's a cat\").  \n",
    "\n",
    "This step is called **feedforward** because data moves **forward** through the network to make a prediction.  \n",
    "\n",
    "üí° **But what if the prediction is wrong?** This is where the next step comes in!  \n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Loss Function ‚Äì Measuring Error  \n",
    "\n",
    "A neural network **doesn‚Äôt start out perfect**‚Äîit makes mistakes!  \n",
    "To improve, we need to measure **how wrong** its predictions are.  \n",
    "\n",
    "A **loss function** is a mathematical formula that tells us **how far off** the network's prediction is from the correct answer.  \n",
    "\n",
    "### Example Loss Functions:  \n",
    "- **Mean Squared Error (MSE)** ‚Äì For regression problems.  \n",
    "- **Cross-Entropy Loss** ‚Äì For classification problems (e.g., cat vs. dog).  \n",
    "\n",
    "The smaller the loss, the better the network‚Äôs predictions!  \n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Backpropagation & Optimization ‚Äì Fixing Mistakes  \n",
    "\n",
    "Now that we know the error, we need to **adjust the weights and biases** to improve the predictions.  \n",
    "\n",
    "### How does backpropagation work?  \n",
    "1. **The error is sent backward through the network** (backpropagation).  \n",
    "2. Each neuron **calculates its contribution to the total error**.  \n",
    "3. The weights and biases are **updated** to reduce the error.  \n",
    "\n",
    "This is done using a technique called **Gradient Descent**!  \n",
    "\n",
    "---\n",
    "\n",
    "## üîß Optimization ‚Äì Updating Weights & Biases  \n",
    "\n",
    "Backpropagation alone is not enough‚Äîwe need an **optimizer** to efficiently adjust the weights and biases.  \n",
    "\n",
    "An **optimizer** is an algorithm that improves the network by minimizing the loss.  \n",
    "\n",
    "### Popular Optimizers:  \n",
    "1. **Gradient Descent** ‚Äì Adjusts weights step by step using derivatives.  \n",
    "2. **Stochastic Gradient Descent (SGD)** ‚Äì Updates after each small batch of data.  \n",
    "3. **Adam (Adaptive Moment Estimation)** ‚Äì The most commonly used optimizer in deep learning.  \n",
    "\n",
    "üí° **Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Practical Deep Learning with PyTorch ‚Äì Part 1  \n",
    "\n",
    "## What is a Tensor? ü§î  \n",
    "\n",
    "In deep learning, **Tensors** are the fundamental building blocks. They are just **multi-dimensional arrays**, like NumPy arrays but more powerful!  \n",
    "\n",
    "A **Tensor** can be:  \n",
    "- A **single number** (scalar)  \n",
    "- A **list of numbers** (vector)  \n",
    "- A **matrix** (2D array)  \n",
    "- A **higher-dimensional array** (3D, 4D, etc.)  \n",
    "\n",
    "üí° **Think of tensors as the data format that neural networks understand!**  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Let's Create Tensors in PyTorch  \n",
    "\n",
    "We‚Äôll use the `torch` library to create some tensors!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import PyTorch\n",
    "\n",
    "# Create a 1D Tensor (Vector)\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4])\n",
    "print(tensor_1d)\n",
    "\n",
    "# Create a 2D Tensor (Matrix)\n",
    "tensor_2d = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor_2d)\n",
    "\n",
    "# Create a Random Tensor\n",
    "tensor_rand = torch.rand(3, 3)  # 3x3 random matrix\n",
    "print(tensor_rand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Play with Tensors in PyTorch  \n",
    "\n",
    "## Your Task:  \n",
    "\n",
    "Let's practice creating and manipulating **tensors** in PyTorch! Try to complete the following tasks before checking the solutions.  \n",
    "\n",
    "### 1Ô∏è‚É£ Create a tensor of shape (2,3) filled with random numbers.  \n",
    "### 2Ô∏è‚É£ Create a tensor of zeros with shape (4,4).  \n",
    "### 3Ô∏è‚É£ Convert a NumPy array into a PyTorch tensor.  \n",
    "\n",
    "üîπ **Hint:** Use functions like `torch.rand()`, `torch.zeros()`, `torch.tensor()`, and `torch.from_numpy()`.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np  # Needed for Task 3\n",
    "\n",
    "# 1Ô∏è‚É£ Create a (2,3) tensor with random numbers\n",
    "tensor_random = torch.rand(2, 3)\n",
    "print(\"Random Tensor (2,3):\\n\", tensor_random)\n",
    "\n",
    "# 2Ô∏è‚É£ Create a (4,4) tensor filled with zeros\n",
    "tensor_zeros = torch.zeros(4, 4)\n",
    "print(\"\\nZero Tensor (4,4):\\n\", tensor_zeros)\n",
    "\n",
    "# 3Ô∏è‚É£ Convert a NumPy array into a PyTorch tensor\n",
    "numpy_array = np.array([[5, 6, 7], [8, 9, 10]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"\\nTensor from NumPy:\\n\", tensor_from_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Building a Simple Neural Network in PyTorch  \n",
    "\n",
    "## üîπ Step 1: Creating a Neural Network (Without Activation Function)  \n",
    "\n",
    "Let's start with a **simple neural network** that takes an input, applies weights and bias, and gives an output. **No activation function yet!**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define input tensor (1 sample, 3 features)\n",
    "x = torch.tensor([[2.0, 3.0, 4.0]])  # Shape: (1,3)\n",
    "\n",
    "# Define weight tensor (3 input features ‚Üí 1 output)\n",
    "w = torch.tensor([[0.1, 0.2, 0.3]])  # Shape: (1,3)\n",
    "\n",
    "# Define bias tensor\n",
    "b = torch.tensor([0.5])  # Shape: (1,)\n",
    "\n",
    "# Compute the output: y = x*w + b\n",
    "y = torch.sum(x * w) + b\n",
    "print(\"Output without activation:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Task 1: Modify Weights & Bias  \n",
    "üîπ Try changing the **values of weights and bias**. What happens to the output? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([[0.5, -0.3, 0.2]])  # Different weights\n",
    "b = torch.tensor([1.0])  # Different bias\n",
    "b = torch.tensor([0.5])  # Shape: (1,)\n",
    "\n",
    "# Compute the output: y = x*w + b\n",
    "y = torch.sum(x * w) + b\n",
    "print(\"Output without activation:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Solution:  \n",
    "When you change the weights, you are modifying the influence of each input.  \n",
    "When you change the bias, you shift the output up or down.  \n",
    "You'll see a **different output**! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Step 2: Adding an Activation Function  \n",
    "\n",
    "Now, let's **add an activation function** (ReLU) to introduce non-linearity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # For activation functions\n",
    "\n",
    "# Apply ReLU activation\n",
    "y_activated = F.relu(y)\n",
    "print(\"Output with ReLU activation:\", y_activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Task 2: Try Different Activation Functions  \n",
    "üîπ Replace `F.relu(y)` with `torch.sigmoid(y)` or `torch.tanh(y)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sigmoid = torch.sigmoid(y)\n",
    "y_tanh = torch.tanh(y)\n",
    "\n",
    "print(\"Sigmoid output:\", y_sigmoid)\n",
    "print(\"Tanh output:\", y_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each activation function **transforms the output differently**!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Step 3: Feedforward  \n",
    "\n",
    "A **feedforward neural network** processes inputs layer by layer to produce an output. In PyTorch, we can define it as a class:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # 3 input features ‚Üí 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # No activation yet\n",
    "\n",
    "# Create model and sample input\n",
    "model = SimpleNN()\n",
    "x = torch.tensor([[2.0, 3.0, 4.0]])\n",
    "output = model(x)\n",
    "print(\"Model output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Task 3: Add an Activation Function  \n",
    "üîπ Modify the network to **include a ReLU activation function** inside `forward()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)  # 3 input features ‚Üí 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "# Create model and sample input\n",
    "model = SimpleNN()\n",
    "x = torch.tensor([[2.0, 3.0, 4.0]])\n",
    "output = model(x)\n",
    "print(\"Model output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Step 4: Loss Function  \n",
    "\n",
    "A **loss function** measures how far the model's predictions are from the actual values. One common choice for regression is **Mean Squared Error (MSE)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target (true output)\n",
    "target = torch.tensor([[10.0]])\n",
    "\n",
    "# Define Mean Squared Error Loss\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Task 4: Try L1 Loss  \n",
    "üîπ Replace `nn.MSELoss()` with `nn.L1Loss()`. What‚Äôs the difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "loss = loss_fn(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MSE Loss** penalizes large errors more.  \n",
    "- **L1 Loss** is more resistant to outliers.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ Step 5: Backpropagation & Optimizer  \n",
    "\n",
    "The optimizer **adjusts the weights and biases** using gradient descent to minimize the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Backpropagation: Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Clear gradients for next iteration\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Task 5: Try the Adam Optimizer  \n",
    "üîπ Change the optimizer to **Adam (`torch.optim.Adam`)** and observe the effect on training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Task: Train a Neural Network on a Custom Dataset  \n",
    "\n",
    "### üîπ Task:\n",
    "Build a dataset where the target function is:  \n",
    "\\[ y = \\frac{e^{(x+1)}}{\\ln(2x)} \\]  \n",
    "Train a neural network on this dataset.\n",
    "\n",
    "### üîπ Hint:\n",
    "- Use PyTorch's `Dataset` class to create the dataset.\n",
    "- Define a neural network with multiple layers.\n",
    "- Use `MSELoss()` as the loss function.\n",
    "- Use `.fit()` or a custom training loop to train the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, size=100):\n",
    "        self.x = torch.linspace(1, 10, size).view(-1, 1)\n",
    "        self.y = (torch.exp(self.x + 1) / torch.log(2 * self.x)).view(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define a simple neural network\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate model, loss, and optimizer\n",
    "model = DeepNN()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
